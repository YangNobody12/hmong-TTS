# ðŸ—£ï¸ Hmong Text-to-Speech (TTS) with Unsloth + LoRA

Hmong-TTS is a high-quality **Text-to-Speech system for the Hmong language**, trained and optimized using **Unsloth** with **LoRA (Robust Low-Rank Adaptation)**. This project aims to provide fast, memory-efficient, and accessible speech models for research, education, and community-based technology.

---

## ðŸš€ Key Features

* âš¡ **1.5Ã— Faster Training & Inference** using Unsloth Optimizations
* ðŸ’¾ **50% Less GPU Memory Usage** with LoRA + Flash Attention 2
* ðŸ”Š Generates **natural Hmong speech** with expressive tone and clarity
* ðŸ” Full support for **lowâ€‘resource training** (small datasets)
* ðŸ§  Supports **Sesame CSM**, **Orpheus**, and **Transformerâ€‘based TTS models** (CrisperWhisper, Spark, etc.)
* ðŸ§© Easy fineâ€‘tuning with LLMâ€‘style adapters for TTS

---

## ðŸ“Œ Why LoRA?

**LoRA (Robust Low-Rank Adaptation)** is a fineâ€‘tuning method designed to be:

* ðŸ§  **Robust on sparse data** â€” ideal for lowâ€‘resource languages like Hmong
* ðŸ§® **Efficient with parameters** â€” trains only small adapter layers
* ðŸ”§ **Compatible with TTS** models that use long audio context

LoRA adapts only the most important latent representations, allowing us to:

> ðŸŽï¸ Train faster â¬†ï¸ â€” ðŸ§  Use less memory â¬‡ï¸ â€” ðŸŽ¤ Maintain speech quality ðŸŽ¯

---

## ðŸ§ª Flash Attention 2 Support

This project uses **Flash Attention 2**, enabling:

* ðŸŽï¸ Faster attention computation
* ðŸ”¥ Less VRAM usage
* ðŸ“ Efficient longâ€‘context speech modeling

**Perfect for TTS**, where long audio sequences are required.

---

## ðŸ“‚ Dataset Format

To fine-tune Hmong TTS using **Unsloth + LoRA**, the dataset must contain:

### **ðŸ§¾ Metadata File (`metadata.csv`)**

Format (pipe-separated):

```
filename|text
```

Example:

```
001.wav|Nyob zoo os, koj nyob li cas?
002.wav|Ua tsaug ntau rau koj qhov kev pab.
```

### **ðŸŽ§ Audio Files**

* Format: **WAV 16-bit PCM**
* Sample rate: **16 kHz or 22.05 kHz**
* Mono channel
* File names must match `metadata.csv`

```
wavs/
 â”œâ”€â”€ 001.wav
 â”œâ”€â”€ 002.wav
 â””â”€â”€ ...
```

### **ðŸ“Œ Optional: JSON Format**

If using JSON, follow this structure:

```json
[
  {
    "audio": "wavs/001.wav",
    "text": "Nyob zoo os, koj nyob li cas?"
  },
  {
    "audio": "wavs/002.wav",
    "text": "Ua tsaug ntau rau koj qhov kev pab."
  }
]
```

ðŸ“ **Tip:** Ensure all text is normalized (no emojis, no special characters, standardized tones if applicable).

---

## ðŸ§  LoRA Architecture Explained

**LoRA (Low-Rank Adaptation)** enables efficient fineâ€‘tuning by adding small trainable layers to large TTS models instead of updating the entire model.

### ðŸ”Ž How LoRA Works

In a transformer layer with weight matrix:

```
W âˆˆ â„(d Ã— k)
```

LoRA adds two small matrices:

```
B âˆˆ â„(d Ã— r)
A âˆˆ â„(r Ã— k)   (where r â‰ª d, k)
```

During fineâ€‘tuning, only **A and B** are trained while the original weights are frozen:

```
W' = W + B Â· A
```

### ðŸŽ¤ Why LoRA for TTS?

| Model Component    | What LoRA Learns                      |
| ------------------ | ------------------------------------- |
| Attention Layer    | Rhythm, tone, syllable timing         |
| MLP / Feedâ€‘forward | Timbre, accent, vocal shape           |
| Decoder Blocks     | Speech smoothness, expressive quality |

ðŸ”Š **Perfect for Hmong**, where training data is limited. LoRA learns important voice traits without overfitting.

### ðŸ”¥ LoRA = Improved LoRA for Lowâ€‘Resource Speech

| Method         | Best For                                  |
| -------------- | ----------------------------------------- |
| LoRA           | Fast & memoryâ€‘efficient fineâ€‘tuning       |
| LoRA           | Lowâ€‘resource languages and noisy datasets |
| LoRA + Unsloth | Fast + Low memory + Robust + High quality |

> ðŸ§  **LoRA is LoRA optimized for sparse and small datasets like Hmong TTS.**

### ðŸ“Œ Summary in 15 Seconds

* LoRA = Add small trainable layers instead of modifying the full model
* Trains **only BA**, freezes base model
* Uses **very little memory**, trains **1.5Ã— faster**
* LoRA = **LoRA built for lowâ€‘resource speech** (ideal for Hmong)

---
